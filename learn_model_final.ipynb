{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyme disease detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5526038941983446001\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15589952717\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12774229198510523427\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:0a:00.0, compute capability: 7.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, UpSampling2D, Cropping2D, concatenate, BatchNormalization, GlobalAveragePooling2D, Conv2DTranspose\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Input\n",
    "#from tensorflow.python.keras import applications\n",
    "#model = applications.VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "config = tf.ConfigProto( device_count = {'GPU': 8 , 'CPU': 32} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "base_model = Xception(input_shape=(600, 600, 3), weights='imagenet', include_top=False)\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "predictions = Dense(2, activation='softmax', name='prediction')(x)\n",
    "model = Model(base_model.input, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Image Generators\n",
    "Based on ImageDataGenerator class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Каталог с данными для обучения\n",
    "train_dir = 'processed_dataset/train'\n",
    "# Каталог с данными для проверки\n",
    "val_dir = 'processed_dataset/val'\n",
    "# Каталог с данными для тестирования\n",
    "test_dir = 'processed_dataset/test'\n",
    "# Размеры изображения\n",
    "img_width, img_height = 600, 600\n",
    "# Размерность тензора на основе изображения для входных данных в нейронную сеть\n",
    "# backend Tensorflow, channels_last\n",
    "input_shape = (img_width, img_height, 3)\n",
    "# Количество эпох\n",
    "epochs = 40\n",
    "# Размер мини-выборки\n",
    "batch_size = 8\n",
    "# Количество изображений для обучения\n",
    "nb_train_samples = 442\n",
    "# Количество изображений для проверки\n",
    "nb_validation_samples = 23\n",
    "# Количество изображений для тестирования\n",
    "nb_test_samples = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 442 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator for checking and adjusting the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning the model using generators\n",
    "\n",
    "train_generator - for learning\n",
    "\n",
    "validation_data - for checking and adjusting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "55/55 [==============================] - 131s 2s/step - loss: 0.6838 - acc: 0.7318 - val_loss: 0.7251 - val_acc: 0.3750\n",
      "Epoch 2/40\n",
      "55/55 [==============================] - 124s 2s/step - loss: 0.6538 - acc: 0.7455 - val_loss: 10.0738 - val_acc: 0.3750\n",
      "Epoch 3/40\n",
      "55/55 [==============================] - 125s 2s/step - loss: 0.5743 - acc: 0.7568 - val_loss: 4.8641 - val_acc: 0.3750\n",
      "Epoch 4/40\n",
      "55/55 [==============================] - 123s 2s/step - loss: 0.5834 - acc: 0.7568 - val_loss: 4.0284 - val_acc: 0.3750\n",
      "Epoch 5/40\n",
      "55/55 [==============================] - 127s 2s/step - loss: 0.5666 - acc: 0.7681 - val_loss: 2.8531 - val_acc: 0.3750\n",
      "Epoch 6/40\n",
      "55/55 [==============================] - 122s 2s/step - loss: 0.5414 - acc: 0.7499 - val_loss: 1.9788 - val_acc: 0.3750\n",
      "Epoch 7/40\n",
      "55/55 [==============================] - 125s 2s/step - loss: 0.6076 - acc: 0.7408 - val_loss: 1.0586 - val_acc: 0.3750\n",
      "Epoch 8/40\n",
      "55/55 [==============================] - 123s 2s/step - loss: 0.5564 - acc: 0.7682 - val_loss: 0.9234 - val_acc: 0.3750\n",
      "Epoch 9/40\n",
      "55/55 [==============================] - 122s 2s/step - loss: 0.5150 - acc: 0.7613 - val_loss: 0.7811 - val_acc: 0.3750\n",
      "Epoch 10/40\n",
      "55/55 [==============================] - 125s 2s/step - loss: 0.5016 - acc: 0.7591 - val_loss: 1.6391 - val_acc: 0.3750\n",
      "Epoch 11/40\n",
      "55/55 [==============================] - 123s 2s/step - loss: 0.5499 - acc: 0.7364 - val_loss: 1.1845 - val_acc: 0.3750\n",
      "Epoch 12/40\n",
      "55/55 [==============================] - 119s 2s/step - loss: 0.5416 - acc: 0.7568 - val_loss: 1.9498 - val_acc: 0.3750\n",
      "Epoch 13/40\n",
      "55/55 [==============================] - 125s 2s/step - loss: 0.5090 - acc: 0.7545 - val_loss: 0.9921 - val_acc: 0.3750\n",
      "Epoch 14/40\n",
      "55/55 [==============================] - 121s 2s/step - loss: 0.5005 - acc: 0.7590 - val_loss: 0.7782 - val_acc: 0.3750\n",
      "Epoch 15/40\n",
      "55/55 [==============================] - 125s 2s/step - loss: 0.5135 - acc: 0.7523 - val_loss: 0.7294 - val_acc: 0.3750\n",
      "Epoch 16/40\n",
      "55/55 [==============================] - 125s 2s/step - loss: 0.4951 - acc: 0.7501 - val_loss: 0.8040 - val_acc: 0.3750\n",
      "Epoch 17/40\n",
      "55/55 [==============================] - 123s 2s/step - loss: 0.4599 - acc: 0.7545 - val_loss: 0.7928 - val_acc: 0.3750\n",
      "Epoch 18/40\n",
      "55/55 [==============================] - 123s 2s/step - loss: 0.4940 - acc: 0.7363 - val_loss: 8.5613 - val_acc: 0.3750\n",
      "Epoch 19/40\n",
      "55/55 [==============================] - 124s 2s/step - loss: 0.4900 - acc: 0.7500 - val_loss: 3.9770 - val_acc: 0.4375\n",
      "Epoch 20/40\n",
      "55/55 [==============================] - 121s 2s/step - loss: 0.4472 - acc: 0.7682 - val_loss: 0.6016 - val_acc: 0.5625\n",
      "Epoch 21/40\n",
      "55/55 [==============================] - 122s 2s/step - loss: 0.4336 - acc: 0.7842 - val_loss: 0.4291 - val_acc: 0.6875\n",
      "Epoch 22/40\n",
      "55/55 [==============================] - 121s 2s/step - loss: 0.4366 - acc: 0.7864 - val_loss: 0.5782 - val_acc: 0.6875\n",
      "Epoch 23/40\n",
      "55/55 [==============================] - 124s 2s/step - loss: 0.4595 - acc: 0.7796 - val_loss: 0.9333 - val_acc: 0.6250\n",
      "Epoch 24/40\n",
      "55/55 [==============================] - 127s 2s/step - loss: 0.4112 - acc: 0.7932 - val_loss: 0.9570 - val_acc: 0.6250\n",
      "Epoch 25/40\n",
      "55/55 [==============================] - 126s 2s/step - loss: 0.4152 - acc: 0.7842 - val_loss: 0.4740 - val_acc: 0.6875\n",
      "Epoch 26/40\n",
      "55/55 [==============================] - 124s 2s/step - loss: 0.4269 - acc: 0.7864 - val_loss: 0.7565 - val_acc: 0.6875\n",
      "Epoch 27/40\n",
      "55/55 [==============================] - 124s 2s/step - loss: 0.3999 - acc: 0.8113 - val_loss: 0.4603 - val_acc: 0.6250\n",
      "Epoch 28/40\n",
      "55/55 [==============================] - 127s 2s/step - loss: 0.3973 - acc: 0.8227 - val_loss: 0.4666 - val_acc: 0.7500\n",
      "Epoch 29/40\n",
      "55/55 [==============================] - 126s 2s/step - loss: 0.4985 - acc: 0.7546 - val_loss: 0.3252 - val_acc: 0.8750\n",
      "Epoch 30/40\n",
      "55/55 [==============================] - 126s 2s/step - loss: 0.4085 - acc: 0.8181 - val_loss: 1.4807 - val_acc: 0.6875\n",
      "Epoch 31/40\n",
      "55/55 [==============================] - 124s 2s/step - loss: 0.4424 - acc: 0.8070 - val_loss: 0.2425 - val_acc: 0.9375\n",
      "Epoch 32/40\n",
      "55/55 [==============================] - 127s 2s/step - loss: 0.3933 - acc: 0.8022 - val_loss: 0.7658 - val_acc: 0.6250\n",
      "Epoch 33/40\n",
      "55/55 [==============================] - 126s 2s/step - loss: 0.3708 - acc: 0.8318 - val_loss: 0.2984 - val_acc: 0.8750\n",
      "Epoch 34/40\n",
      "55/55 [==============================] - 126s 2s/step - loss: 0.4416 - acc: 0.7934 - val_loss: 0.5554 - val_acc: 0.6250\n",
      "Epoch 35/40\n",
      "55/55 [==============================] - 125s 2s/step - loss: 0.4075 - acc: 0.8229 - val_loss: 0.3986 - val_acc: 0.7500\n",
      "Epoch 36/40\n",
      "55/55 [==============================] - 127s 2s/step - loss: 0.3844 - acc: 0.8023 - val_loss: 0.1653 - val_acc: 1.0000\n",
      "Epoch 37/40\n",
      "55/55 [==============================] - 126s 2s/step - loss: 0.3448 - acc: 0.8364 - val_loss: 0.5486 - val_acc: 0.9375\n",
      "Epoch 38/40\n",
      "55/55 [==============================] - 127s 2s/step - loss: 0.3906 - acc: 0.7931 - val_loss: 0.3661 - val_acc: 0.6875\n",
      "Epoch 39/40\n",
      "55/55 [==============================] - 128s 2s/step - loss: 0.4099 - acc: 0.8000 - val_loss: 0.4441 - val_acc: 0.7500\n",
      "Epoch 40/40\n",
      "55/55 [==============================] - 124s 2s/step - loss: 0.3533 - acc: 0.8295 - val_loss: 0.3386 - val_acc: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7902b44310>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the accuracy score of the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 81.25%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate_generator(test_generator, nb_test_samples // batch_size)\n",
    "print(\"Accuracy on the test set: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the name of the output node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'prediction/Softmax:0' shape=(?, 2) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "print(model.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed a random Tensor to make sure the system is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.96820104, 0.03179901]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "model.predict(np.random.rand(1, 600, 600, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to predict on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9974831 , 0.00251691]], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.keras.preprocessing import image\n",
    "\n",
    "x = image.load_img('processed_dataset/test/EM/bull_16.jpg', target_size=(600,600))\n",
    "x = image.img_to_array(x)\n",
    "x = x.reshape((1,) + x.shape)\n",
    "x = x/255.\n",
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_600x600_83perc.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
